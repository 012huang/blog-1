# Investigating performance of Apache Arrow (Java) in-memory (part 3)

In the first two part of the series (see 
[part 1](https://github.com/animeshtrivedi/blog/blob/master/post/2018-10-03-arrow-binary.md), 
[part 2](https://github.com/animeshtrivedi/blog/blob/master/post/2018-10-09-arrow-int.md)), 
I showed the basic performance and optimization knobs for integer and binary schema. 

In this part 3 of the blog series, I am going to attempt the answer the question - how far away
the Java performance is from the C++ code? That will give us the performance limit that we can
realistically target.


As usual the benchmark code is available 
at [https://github.com/animeshtrivedi/benchmarking-arrow](https://github.com/animeshtrivedi/benchmarking-arrow). 
Its README.md might not be up-to date yet, so please have a look at the source code when in doubt.
Unfortunately this blog was done using multiple commits from November, 2018. Just browse the code, otherwise write to me. 

 ## Index 
 1. [Recap](#recap)
     - 1.1 [Generating data](#generating-data)
     - 1.2 [Running the Java Benchmark](#running-the-java-benchmark)
 2. [C++ Benchmark](#c-benchmark)     
 3. [Comparing with Java Code](#comparing-with-java-code)     
     - 3.1 [No-Null Values Optimization](#no-null-values-optimization)
     - 3.2 [C++ bitmap optimization](#c-bitmap-optimization)
     - 3.3 [Data copies](#data-copies)
     - 3.4 [C++ Optimizations](#c-optimizations)          
 4. [Conclusions](#conclusions)
 5. [Thanks](#thanks)
 
 ## Recap
 Let us recap by establishing the last known performance numbers from reading an integer schema. We
 will setup the benchmark, generate data, and re-run the Java code be sure. 
 
 ### Generating data 
 We first generate a large arrow data file (we now have the capability). We will generate a file with 
 10 billion integers, hence giving the file size of ~40GB. We will use all flags discussed previously. 
 
```
numactl -C 0 -m 0 java -XX:+UseG1GC -Xmn200G -Xmx256G \
-Ddrill.enable_unsafe_memory_access=true -Dio.netty.buffer.bytebuf.checkAccessible=false \
-cp ./4j/:./benchmark-arrow-1.0.jar:./dependency/*  com.github.animeshtrivedi.benchmark.Main \
-t DataGen -p 1 -r 10000000000 -c 1 -g 1000000 -n int -o /mnt/nvme0n1/arrow/
```

After a successful data generation, there will be a file called `/mnt/nvme0n1/arrow/part-0`. We will 
use this file for testing.

### Running the Java Benchmark 
 
Single core performance when data is held in memory after reading it from the file (generated in the 
previous step). We will benchmark the three kind of readers (default, holder, and directly using the 
buffers using the unsafe API) as: 

```
numactl -C 0 -m 0 \
java -XX:+UseG1GC -Xmn200G -Xmx256G \
-Ddrill.enable_unsafe_memory_access=true -Dio.netty.buffer.bytebuf.checkAccessible=false \
-cp ./4j/:./benchmark-arrow-1.0.jar:./dependency/* com.github.animeshtrivedi.benchmark.Main \
-p 1 -i /mnt/nvme0n1/arrow/ -it arrow -e [default|holder|unsafe]
```
 
**Table 1 - [Java] single core performance when data is read from a file**

| Reader Type       |  Bandwidth | 
| :------------- |:-------------|
| default	 |  9.00     Gbps |  
| holder     |  10.54    Gbps |
| unsafe     |  12.56    Gbps |

It is a bit off from the last reported number where data is generated in memory. I don't know why yet. You can repeat the previous experiments as : 

```
numactl -C 0 -m 0 \
java -XX:+UseG1GC -Xmn200G -Xmx256G \
-Ddrill.enable_unsafe_memory_access=true -Dio.netty.buffer.bytebuf.checkAccessible=false \
-cp ./4j/:./benchmark-arrow-1.0.jar:./dependency/* com.github.animeshtrivedi.benchmark.Main \
-t ArrowMemBench -p 1 -r 10000000000 -c 1 -g 1000000 -n int -e [default|holder|unsafe]
```

**Table 2 - [Java] single core performance with in-memory data generation**

| Reader Type       |  Bandwidth | 
| :------------- |:-------------|
| default	 |  9.70      Gbps |  
| holder     |  11.13     Gbps |
| unsafe     |  13.63     Gbps |


**Note** that in both cases, the benchmark is always done with data held in memory. The only difference is 
where the data came from (generated or read from a file).
 
## C++ Benchmark
We now write an equivalent benchmark in C++ to benchmark its performance. I coded up an example where 
data is read from a file. The new code is avaiblale in the `cpp/` folder of the project. Unfortunately 
the project is little messed up and and most of these experiments are compile time setups. I will clean 
up the code soon. 
 
The benchmarking code is at :  [https://github.com/animeshtrivedi/benchmarking-arrow/blob/master/cpp/src/arrow-reader.cpp#L126](https://github.com/animeshtrivedi/benchmarking-arrow/blob/master/cpp/src/arrow-reader.cpp#L126).


In the C++ code, there are two ways in which you can read a file (1) using file I/O; or (2) using mmap. 
The mmap code (`cpp/src/arrow/io/file.cc`) in the arrow c++ source uses `MAP_PRIVATE` flags. Naturally not 
all pages are brought in when are running our benchmark. If I modify the code to use `MAP_SHARED|MAP_POPULATE`
flags then we can separate the cost of reading a file, from the benchmarking code. This code is denoted
by `mmap-populate` row. 

Here is the performance in all three cases

**Table 3 - [C++] single core performance**

| Reader Type       |  Bandwidth | x better than Java code |
| :------------- |:-------------|:-------------| 
| file I/O |  45.4 Gbps | 3.62x  |
| mmap     |  66.1 Gbps | 5.28x |
| mmap-populate     |  85.7 Gbps | 6.85x | 
 

So, clearly the C++ code is significantly faster than the Java code. Now we will investigate why and
what can we do to optimize the Java code. 

A few things worth noticing about the C++ code 

  * In our benchmark, accesses to C++ class variable are memory accesses. Somehow, these are not brought in a register. So, if possible use a local stack variable that can be put in a register, and then assigned the final value of the local variable to the class member. For example, you see this pattern how I count number of integers in the schema. An earlier version of   the code has `this->int_count++` in the loop, which was bad. The performance loss was more than 50%.   
  * The same concept applies to making functions `const` - i.e., if a function does not modify a class member 
  then consider declaring it constant. 

## Comparing with Java Code

We now try to understand what is happening with the Java code. Here are my findings. 

### No-Null Values Optimization 

The C++ code's NULL value bit check condition looks something like: 
```cpp
bool IsValid(int64_t i) const {
  return null_bitmap_data_ == NULLPTR ||
         BitUtil::GetBit(null_bitmap_data_, i + data_->offset);
}
```

from here: [https://github.com/apache/arrow/blob/apache-arrow-0.11.0/cpp/src/arrow/array.h#L212](https://github.com/apache/arrow/blob/apache-arrow-0.11.0/cpp/src/arrow/array.h#L212)

The code that we have been testing with C++ so far has all valid integers branch. So the validity 
check branch is always optimized away. We now introduce another flag in the Java data generation 
code `-l`, that will set one integer value as null in each batch. With this change in the code, 
we set to revise the performance for Java and C++ code. 


**Table 4 - [C++] single core performance with null values from a file**

| Reader Type  |  Bandwidth (no null) | Bandwidth (null values) 
| :------------- |:-------------|:-------------| 
| file I/O |  45.4 Gbps | 14.7 Gbps |
| mmap     |  66.1 Gbps | 17.1 Gbps |
| mmap-populate     |  85.7 Gbps | 19.5 Gbps |  


**Table 5 - [Java] single core performance with null values from a file**

| Reader Type       |  Bandwidth (no null) | Bandwidth (null values)
| :------------- |:-------------|:-------------| 
| default |  8.9 Gbps | 9.0 Gbps |
| holder     |  10.54 Gbps | 10.35 Gbps |
| unsafe |  12.56 Gbps | 12.48 Gbps |  

As you can see there is a massive performance collapse in the C++ code, which was optimizing the
branch for the validity check when all values are valid. 

### C++ bitmap optimization
Though we can improve the C++ bitmap check code. The code for bit checks looks like this

```cpp
static inline bool GetBit(const uint8_t* bits, int64_t i) {
  return (bits[i / 8] & kBitmask[i % 8]) != 0;
}
```
from : [https://github.com/apache/arrow/blob/apache-arrow-0.11.0/cpp/src/arrow/util/bit-util.h#L318](https://github.com/apache/arrow/blob/apache-arrow-0.11.0/cpp/src/arrow/util/bit-util.h#L318)

The problematic bit here is that `int64_t` is a signed number, and modulo and 
division on signed numbers require more operations than the unsigned one. Here is 
 an example 
 
 ```cpp
 // Type your code here, or load an example.
 #include <stdint.h> 
 
 int with_signed(int64_t a) {
     return a % 8;
 }
 
 int with_unsigned(unsigned long a) {
     return a % 8;
 }
 ```

put this code in [https://godbolt.org/](https://godbolt.org/) and you get
 
 ```asm
 with_signed(long):
         mov     rdx, rdi
         sar     rdx, 63
         shr     rdx, 61
         lea     rax, [rdi+rdx]
         and     eax, 7
         sub     rax, rdx
         ret
 with_unsigned(unsigned long):
         mov     eax, edi
         and     eax, 7
         ret
 ```
quick reference: [https://godbolt.org/z/1aDi4r](https://godbolt.org/z/1aDi4r).

So if we optimize these routines as 
```cpp
static inline bool GetBit(const uint8_t* bits, int64_t i) {
  return (bits[i >> 3] & kBitmask[i & 0x7]) != 0;
}
```

or make `int64_t` `unsigned long` as it should be because it is used as a counter,
we get performance improvements as: 

**Table 6 - [C++] single core performance with null values from a file**

| Reader Type  |  Bandwidth (no null) | Bandwidth (null values) | bitmap-optimized with null values 
| :------------- |:-------------|:-------------|:-------------|
| file I/O |  45.4 Gbps | 14.7 Gbps | 20.1 Gbps | 
| mmap     |  66.1 Gbps | 17.1 Gbps | 27.4 Gbps | 
| mmap-populate     |  85.7 Gbps | 19.5 Gbps | 30.4 Gbps |   

OR - alternatively if signed to unsigned upgrade is not possible (perhaps in every language), then 
in the C++ code, we should use the bitmap operations directory ( `<<3` for division by 8, and ` & 0x7` 
for modulo by 8 operation). I am going to open a pull request for these changes. 

### Data copies
The number of data copies in the C++ code and Java codes are different. Even though Java also have an `mmap` function 
([https://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html](https://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html)),
it is limited to 2GB memory region, which make it difficult to use with large files as we have. Furthermore, 
Arrow's mmap implementation actually shares memory and does not do copies when passing to the reader 
(see here [https://github.com/apache/arrow/blob/apache-arrow-0.11.0/cpp/src/arrow/io/file.cc#L531](https://github.com/apache/arrow/blob/apache-arrow-0.11.0/cpp/src/arrow/io/file.cc#L531) vs
[https://github.com/apache/arrow/blob/apache-arrow-0.11.0/cpp/src/arrow/io/file.cc#L209](https://github.com/apache/arrow/blob/apache-arrow-0.11.0/cpp/src/arrow/io/file.cc#L209)). 

So realistically speaking, the performance target for the Java code should be file I/O 
performance, not the mmap performance. 

However, the file routine in C++ code has both file reading as well as arrow logic. What we are interested 
is the arrow logic. So we wrote our own in-memory data holder. And use that to benchmark the performance. 
You can see the code this here []().


**Table 6 - [C++] My implementation of InMemory data holder**

| Reader Type  |  Bandwidth (no null) | Bandwidth (null values) with bitmap-optimized  
| :------------- |:-------------|:-------------|
| file I/O |  45.4 Gbps | 20.1 Gbps | 
| mmap     |  66.1 Gbps | 27.4 Gbps | 
| mmap-populate     |  85.7 Gbps | 30.4 Gbps |   
| InMemory | 48.7 Gbps | 21.5 Gbps | 

So, the key take away here is that, Java code should be compared against the InMemory implementation, 
which is doing the exactly same thing. 

### C++ optimizations  

Note: this is an on-going investigation. I have some high-level results that I am sharing 
here. These items needs more comprehensive investigation. 

Next thing to consider on our list is C++ related optimization which there are many. 
What I am trying to isolate is that given the data access pattern of Arrow (one int 
value vector and associated bitmap with some null values), how fast C++ and Java 
codes can count number of valid integers and perform a checksum. The code for 
this standalone investigation is at 
[https://github.com/animeshtrivedi/java-cpp-fun](https://github.com/animeshtrivedi/java-cpp-fun).
This work is very much in progress. However there are three items that stand out from 
our micro-micro benchmark: 
 
   * First, loop unrolling. GCC can unroll loops and optimize them significantly. 
   So far I have not seen JVM do this in a smart manner. 
   * As previously discussed, when we look at the JITTed assembly for the benchmark, 
   we notice that Java code always does memory reference for the class variables, instead 
   of keeping variables in register.
   * The kind of JVM we use. For example, Azul JVM gave 30-50% performance gains over 
   the stock Oracle/JVM for our standalone micro-benchmark.

These items needs to be investigated more comprehensively. But for now we roughly understand 
the performance gap. I will write another blog post about these issues later. 


## Conclusions 
  * The `all valid value` branch optimization for bitmaps is missing from the Java implementation. 
  * Arrow C++ bitmap checks can be optimized to use unsigned values to make sure that `/` and `%` operation 
  compile to efficient bitmap routines. 
  * Given all things considered, Arrow java code (for a single column integer schema with null values) is 
  almost 2x off from the C++ code. We can compre the java-unsafe performance with the C++ InMemory row. 
  There are a lots of area where small things such as JITting process and code optimizations will kick in, 
  but for now we stop here when we understand the performance gap. Here is the quick recap of the best 
  numbers that I can extract 
  
| Reader Type  |  Bandwidth (no null) | Bandwidth (null values)
| :------------- |:-------------|:-------------|
| [c++] file I/O |  45.4 Gbps | 20.1 Gbps | 
| [c++] mmap     |  66.1 Gbps | 27.4 Gbps | 
| [c++] mmap-populate     |  85.7 Gbps | 30.4 Gbps |   
| [c++] InMemory | 48.7 Gbps | 21.5 Gbps | 
| [java] unsafe |  12.56 Gbps | 12.48 Gbps |
| [java] unsafe with all valid optimization |  **23.88** Gbps | 12.48 Gbps |

## Thanks
Thanks [@pepperjo](https://github.com/PepperJo) for helping me to debug the performance issues. I used [JITWatch](https://github.com/AdoptOpenJDK/jitwatch) to look through the JIT assembly, and got help from [@chriswhocodes](https://github.com/chriswhocodes).
